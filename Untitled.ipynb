{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f2455ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc3745ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda) #10.1\n",
    "t3 = time.time()\n",
    "##############################################################################\n",
    "\"\"\"args for AE\"\"\"\n",
    "\n",
    "args = {}\n",
    "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
    "args['n_channel'] = 1#3    # number of channels in the input data \n",
    "\n",
    "args['n_z'] = 300 #600     # number of dimensions in latent space. \n",
    "\n",
    "args['sigma'] = 1.0        # variance in n_z\n",
    "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
    "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
    "args['epochs'] = 6      # how many epochs to run for\n",
    "args['batch_size'] = 100   # batch size for SGD\n",
    "args['save'] = True        # save weights at each epoch of training if True\n",
    "args['train'] = True       # train networks if True, else load networks from\n",
    "\n",
    "args['dataset'] = 'mnist'  #'fmnist' # specify which dataset to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc7b4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pd_speech_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cebb13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77c2791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(df.columns) - 1\n",
    "num_targets_0 = 150\n",
    "hidden_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06868cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse  tensor(1.6068e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.1458e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.5864e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.6979e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.7889e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.2916e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8796e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.6534e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 0 \tTrain Loss: 40245212816273833984.000000 \tmse loss: 18653617109166194688.000000 \tmse2 loss: 21591595170236727296.000000\n",
      "Saving..\n",
      "mse  tensor(1.4332e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.7777e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.7069e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.2770e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.7557e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.4423e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.6186e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.6278e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 1 \tTrain Loss: 40447009672833007616.000000 \tmse loss: 18653617371159199744.000000 \tmse2 loss: 21793392610911453184.000000\n",
      "mse  tensor(2.2534e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.3764e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.0347e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9252e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.7041e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9646e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.6280e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8510e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 2 \tTrain Loss: 44909301696319979520.000000 \tmse loss: 18653617624562270208.000000 \tmse2 loss: 26255684273621172224.000000\n",
      "mse  tensor(2.1097e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.1675e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.5839e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9934e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.1996e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.0254e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.5284e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.3482e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 3 \tTrain Loss: 44844432108009029632.000000 \tmse loss: 18653617624562270208.000000 \tmse2 loss: 26190814711080026112.000000\n",
      "mse  tensor(1.9768e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.7729e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9750e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.3844e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.1114e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8139e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9911e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.6025e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 4 \tTrain Loss: 43273356504473796608.000000 \tmse loss: 18653618054058999808.000000 \tmse2 loss: 24619738141177151488.000000\n",
      "mse  tensor(2.0871e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8152e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.7536e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.3947e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8880e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.1949e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9890e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.4292e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 5 \tTrain Loss: 34663281110015803392.000000 \tmse loss: 18653617057626587136.000000 \tmse2 loss: 16009664159763398656.000000\n",
      "Saving..\n",
      "./MNIST/models-tab/crs5/0/f_enc.pth\n",
      "./MNIST/models-tab/crs5/0/f_dec.pth\n",
      "\n",
      "total time(min): 0.49\n",
      "mse  tensor(1.7323e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8335e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.0781e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.2025e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9480e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.7340e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.6097e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(3.1871e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 0 \tTrain Loss: 37382200192424476672.000000 \tmse loss: 18653617229425278976.000000 \tmse2 loss: 18728582520617566208.000000\n",
      "Saving..\n",
      "mse  tensor(1.9057e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9845e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.5902e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.0001e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8352e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.6205e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.5924e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.4897e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 1 \tTrain Loss: 40925266204071821312.000000 \tmse loss: 18653617469943447552.000000 \tmse2 loss: 22271648828617654272.000000\n",
      "mse  tensor(1.5510e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.1815e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.5529e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.1215e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9301e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.0962e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.6826e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.4412e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 2 \tTrain Loss: 45068553654816997376.000000 \tmse loss: 18653617083396390912.000000 \tmse2 loss: 26414937310154981376.000000\n",
      "mse  tensor(1.9376e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.0796e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8864e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.0409e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.5118e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8488e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.4022e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.1707e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 3 \tTrain Loss: 47887033412041572352.000000 \tmse loss: 18653616860058091520.000000 \tmse2 loss: 29233416551983480832.000000\n",
      "mse  tensor(1.6274e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.5990e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8855e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.0509e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9760e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8624e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.6312e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.3045e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 4 \tTrain Loss: 41048410475590582272.000000 \tmse loss: 18653617495713251328.000000 \tmse2 loss: 22394792215373152256.000000\n",
      "mse  tensor(2.1766e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9881e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.6155e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.8399e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.0152e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.9501e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(2.4782e+17, grad_fn=<MseLossBackward>)\n",
      "mse  tensor(1.5344e+17, grad_fn=<MseLossBackward>)\n",
      "Epoch: 5 \tTrain Loss: 40506804688564256768.000000 \tmse loss: 18653618002519392256.000000 \tmse2 loss: 21853186900793229312.000000\n",
      "./MNIST/models-tab/crs5/1/f_enc.pth\n",
      "./MNIST/models-tab/crs5/1/f_dec.pth\n",
      "\n",
      "total time(min): 0.49\n"
     ]
    }
   ],
   "source": [
    "dtrn_iris = './MNIST/trn_iris/'\n",
    "ids = os.listdir(dtrn_iris)\n",
    "\n",
    "x = df.drop('class', axis=1).values\n",
    "y = df['class'].values\n",
    "tensor_x = torch.Tensor(np.array(x))\n",
    "tensor_y = torch.Tensor(np.array(y))\n",
    "\n",
    "\n",
    "#for i in range(5):\n",
    "for i in range(len(ids)):\n",
    "    #print()\n",
    "    #print(i)\n",
    "    encoder = Encoder(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets_0,\n",
    "        hidden_size=hidden_size\n",
    "    )\n",
    "    decoder = Decoder(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets_0,\n",
    "        hidden_size=hidden_size\n",
    "    )\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    decoder = decoder.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "    #decoder loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    batch_size = 100\n",
    "    num_workers = 0\n",
    "\n",
    "    mnist_bal = TensorDataset(tensor_x,tensor_y)\n",
    "    train_loader = torch.utils.data.DataLoader(mnist_bal, \n",
    "        batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "    \n",
    "    best_loss = np.inf\n",
    "\n",
    "    t0 = time.time()\n",
    "    if args['train']:\n",
    "        enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
    "        dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
    "    \n",
    "        for epoch in range(args['epochs']):\n",
    "            train_loss = 0.0\n",
    "            tmse_loss = 0.0\n",
    "            tdiscr_loss = 0.0\n",
    "            # train for one epoch -- set nets to train mode\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            for images,labs in train_loader:\n",
    "            \n",
    "                # zero gradients for each batch\n",
    "                encoder.zero_grad()\n",
    "                decoder.zero_grad()\n",
    "                images, labs = images.to(device), labs.to(device)\n",
    "                #import pdb; pdb.set_trace()\n",
    "                labsn = labs.detach().cpu().numpy()\n",
    "\n",
    "                # run images\n",
    "                z_hat = encoder(images)\n",
    "#                import pdb; pdb.set_trace()\n",
    "                x_hat = decoder(z_hat) #decoder outputs tanh\n",
    "\n",
    "                mse = criterion(x_hat,images)\n",
    "                print('mse ',mse)\n",
    "                resx = []\n",
    "                resy = []\n",
    "                \n",
    "                #tc = np.random.choice(1,0)\n",
    "                tc = random.choice([0, 1])\n",
    "                xbeg = df[df['class'] == tc].drop('class', axis=1).values\n",
    "                ybeg = df[df['class'] == tc]['class'].values\n",
    "\n",
    "                xlen = len(xbeg)\n",
    "                nsamp = min(xlen, 100)\n",
    "                ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
    "                xclass = xbeg[ind]\n",
    "                yclass = ybeg[ind]\n",
    "                \n",
    "                xclen = len(xclass)\n",
    "                xcminus = np.arange(1,xclen)\n",
    "                xcplus = np.append(xcminus,0)\n",
    "                \n",
    "                xcnew = (xclass[[xcplus],:])\n",
    "                xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2])\n",
    "            \n",
    "                xcnew = torch.Tensor(xcnew)\n",
    "                xcnew = xcnew.to(device)\n",
    "            \n",
    "                #encode xclass to feature space\n",
    "                xclass = torch.Tensor(xclass)\n",
    "                xclass = xclass.to(device)\n",
    "                xclass = encoder(xclass)\n",
    "            \n",
    "                xclass = xclass.detach().cpu().numpy()\n",
    "            \n",
    "                xc_enc = (xclass[[xcplus],:])\n",
    "                xc_enc = np.squeeze(xc_enc)\n",
    "            \n",
    "                xc_enc = torch.Tensor(xc_enc)\n",
    "                xc_enc = xc_enc.to(device)\n",
    "                \n",
    "                ximg = decoder(xc_enc)\n",
    "                \n",
    "                mse2 = criterion(ximg,xcnew)\n",
    "            \n",
    "                comb_loss = mse2 + mse\n",
    "                comb_loss.backward()\n",
    "            \n",
    "                enc_optim.step()\n",
    "                dec_optim.step()\n",
    "            \n",
    "                train_loss += comb_loss.item()*images.size(0)\n",
    "                tmse_loss += mse.item()*images.size(0)\n",
    "                tdiscr_loss += mse2.item()*images.size(0)\n",
    "            \n",
    "                 \n",
    "            # print avg training statistics \n",
    "            train_loss = train_loss/len(train_loader)\n",
    "            tmse_loss = tmse_loss/len(train_loader)\n",
    "            tdiscr_loss = tdiscr_loss/len(train_loader)\n",
    "            print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
    "                    train_loss,tmse_loss,tdiscr_loss))\n",
    "            \n",
    "                   \n",
    "            #store the best encoder and decoder models\n",
    "            #here, /crs5 is a reference to 5 way cross validation, but is not\n",
    "            #necessary for illustration purposes\n",
    "            if train_loss < best_loss:\n",
    "                print('Saving..')\n",
    "                path_enc = './MNIST/models-tab/crs5/' \\\n",
    "                    + str(i) + '/bst_enc.pth'\n",
    "                path_dec = './MNIST/models-tab/crs5/' \\\n",
    "                    + str(i) + '/bst_dec.pth'\n",
    "             \n",
    "                torch.save(encoder.state_dict(), path_enc)\n",
    "                torch.save(decoder.state_dict(), path_dec)\n",
    "        \n",
    "                best_loss = train_loss\n",
    "            \n",
    "        #in addition, store the final model (may not be the best) for\n",
    "        #informational purposes\n",
    "        path_enc = './MNIST/models-tab/crs5/' \\\n",
    "            + str(i) + '/f_enc.pth'\n",
    "        path_dec = './MNIST/models-tab/crs5/' \\\n",
    "            + str(i) + '/f_dec.pth'\n",
    "        print(path_enc)\n",
    "        print(path_dec)\n",
    "        torch.save(encoder.state_dict(), path_enc)\n",
    "        torch.save(decoder.state_dict(), path_dec)\n",
    "        print()\n",
    "              \n",
    "    t1 = time.time()\n",
    "    print('total time(min): {:.2f}'.format((t1 - t0)/60))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b6cad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        cha_1 = 256\n",
    "        cha_2 = 512\n",
    "        cha_3 = 512\n",
    "\n",
    "        cha_1_reshape = int(hidden_size/cha_1)\n",
    "        cha_po_1 = int(hidden_size/cha_1/2)\n",
    "        cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
    "\n",
    "        self.cha_1 = cha_1\n",
    "        self.cha_2 = cha_2\n",
    "        self.cha_3 = cha_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "\n",
    "        self.conv_t2 = nn.ConvTranspose1d(in_channels=cha_2, out_channels=cha_2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
    "        \n",
    "        self.conv_t1 = nn.ConvTranspose1d(in_channels=cha_1, out_channels=cha_1, kernel_size=5, stride=1, padding=2, bias=False)\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
    "\n",
    "        self.dense1 = nn.Linear(hidden_size, num_features)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_t2(x)            # input = [100, 512, 8], output = [100, 512, 8]\n",
    "        x = F.relu(x)                  # input = [100, 512, 8], output = [100, 512, 8]\n",
    "        x = self.batch_norm_c2(x)      # input = [100, 512, 8], output = [100, 512, 8]\n",
    "        \n",
    "        x = x.reshape(x.shape[0], 256, 16)    # input = [100, 512, 8], output = [100, 256, 16] \n",
    "                                       #   can't the output_size be increased from 8 to 16 instead of a reshape?\n",
    "        \n",
    "        x = self.conv_t1(x)            # input = [100, 256, 16], output = [100, 256, 16]\n",
    "        x = F.relu(x)                  # input = [100, 256, 16], output = [100, 256, 16]\n",
    "        x = self.batch_norm_c1(x)      # input = [100, 256, 16], output = [100, 256, 16]\n",
    "\n",
    "        x = x.reshape(x.shape[0], hidden_size)   # input = [100, 256, 16], output = [100, 4096]\n",
    "        x = self.dense1(x)                       # input = [100, 4096], output = [100, 753]\n",
    "        x = self.batch_norm1(x)                  # input = [100, 753], output = [100, 753]\n",
    "        x = F.celu(x, alpha=0.06)                # input = [100, 753], output = [100, 753]\n",
    "        x = nn.Tanh()(x)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81ca72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        cha_1 = 256\n",
    "        cha_2 = 512\n",
    "        cha_3 = 512\n",
    "\n",
    "        cha_1_reshape = int(hidden_size/cha_1)\n",
    "        cha_po_1 = int(hidden_size/cha_1/2)\n",
    "        cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
    "\n",
    "        self.cha_1 = cha_1\n",
    "        self.cha_2 = cha_2\n",
    "        self.cha_3 = cha_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, hidden_size)\n",
    "\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
    "        self.conv1 = nn.Conv1d(cha_1, cha_2, kernel_size=5, stride=1, padding=2, bias=False)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
    "\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
    "        self.conv2 = nn.Conv1d(cha_2, cha_2, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "\n",
    "        self.batch_norm_c3 = nn.BatchNorm1d(cha_2)\n",
    "        self.conv2 = nn.Conv1d(cha_2, cha_2, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.batch_norm1(x)          # input = [100, 753],  output = [100, 753]\n",
    "        x = self.dense1(x)               # input = [100, 753],  output = [100, 4096]\n",
    "        x = F.celu(x, alpha=0.06)        # input = [100, 4096], output = [100, 4096]\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape) # input =  [100, 4096], output = [100, 256, 16]\n",
    "\n",
    "        x = self.batch_norm_c1(x) # input = [100, 256, 16], output = [100, 256, 16]\n",
    "        x = self.conv1(x)         # input = [100, 256, 16], output = [100, 512, 16]\n",
    "        x = F.relu(x)             # input = [100, 512, 16], output = [100, 512, 16]\n",
    "        \n",
    "        x = self.ave_po_c1(x)     # input = [100, 512, 16], output = [100, 512, 8]\n",
    "\n",
    "        x = self.batch_norm_c2(x) # input = [100, 512, 8], output = [100, 512, 8]\n",
    "        x = self.conv2(x)         # input = [100, 512, 8], output = [100, 512, 8]\n",
    "        x = F.relu(x)             # input = [100, 512, 8], output = [100, 512, 8]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_get_class1(c):    \n",
    "    xbeg = dec_x[dec_y == c]\n",
    "    ybeg = dec_y[dec_y == c]\n",
    "    \n",
    "    return xbeg, ybeg\n",
    "    #return xclass, yclass\n",
    "\n",
    "def G_SM1(X, y,n_to_sample,cl):\n",
    "    # fitting the model\n",
    "    n_neigh = 5 + 1\n",
    "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
    "    nn.fit(X)\n",
    "    dist, ind = nn.kneighbors(X)\n",
    "\n",
    "    # generating samples\n",
    "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
    "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
    "\n",
    "    X_base = X[base_indices]\n",
    "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
    "\n",
    "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
    "            X_neighbor - X_base)\n",
    "\n",
    "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
    "    return samples, [cl]*n_to_sample\n",
    "\n",
    "#############################################################################\n",
    "np.printoptions(precision=5,suppress=True)\n",
    "\n",
    "dtrnimg = '.../0_trn_img.txt'\n",
    "dtrnlab = '.../0_trn_lab.txt'\n",
    "\n",
    "ids = os.listdir(dtrnimg)\n",
    "idtri_f = [os.path.join(dtrnimg, image_id) for image_id in ids]\n",
    "print(idtri_f)\n",
    "\n",
    "ids = os.listdir(dtrnlab)\n",
    "idtrl_f = [os.path.join(dtrnlab, image_id) for image_id in ids]\n",
    "print(idtrl_f)\n",
    "\n",
    "#path on the computer where the models are stored\n",
    "modpth = '.../MNIST/models-tab/crs5/'\n",
    "\n",
    "encf = []\n",
    "decf = []\n",
    "for p in range(5):\n",
    "    enc = modpth + '/' + str(p) + '/bst_enc.pth'\n",
    "    dec = modpth + '/' + str(p) + '/bst_dec.pth'\n",
    "    encf.append(enc)\n",
    "    decf.append(dec)\n",
    "\n",
    "for m in range(5):\n",
    "    print(m)\n",
    "    trnimgfile = idtri_f[m]\n",
    "    trnlabfile = idtrl_f[m]\n",
    "    print(trnimgfile)\n",
    "    print(trnlabfile)\n",
    "    dec_x = np.loadtxt(trnimgfile) \n",
    "    dec_y = np.loadtxt(trnlabfile)\n",
    "\n",
    "    print('train imgs before reshape ',dec_x.shape) #(44993, 3072) 45500, 3072)\n",
    "    print('train labels ',dec_y.shape) #(44993,) (45500,)\n",
    "\n",
    "    dec_x = dec_x.reshape(dec_x.shape[0],1,28,28)\n",
    "\n",
    "    print('decy ',dec_y.shape)\n",
    "    print(collections.Counter(dec_y))\n",
    "    \n",
    "    print('train imgs after reshape ',dec_x.shape) #(45000,3,32,32)\n",
    "\n",
    "    classes = ('0', '1', '2', '3', '4',\n",
    "           '5', '6', '7', '8', '9')\n",
    "    \n",
    "    #generate some images \n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    path_enc = encf[m]\n",
    "    path_dec = decf[m]\n",
    "\n",
    "    encoder = Encoder(args)\n",
    "    encoder.load_state_dict(torch.load(path_enc), strict=False)\n",
    "    encoder = encoder.to(device)\n",
    "\n",
    "    decoder = Decoder(args)\n",
    "    decoder.load_state_dict(torch.load(path_dec), strict=False)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    #imbal = [4500, 2000, 1000, 800, 600, 500, 400, 250, 150, 80]\n",
    "    imbal = [4000, 2000, 1000, 750, 500, 350, 200, 100, 60, 40]\n",
    "\n",
    "    resx = []\n",
    "    resy = []\n",
    "\n",
    "    for i in range(1,10):\n",
    "        xclass, yclass = biased_get_class1(i)\n",
    "        print(xclass.shape) #(500, 3, 32, 32)\n",
    "        print(yclass[0]) #(500,)\n",
    "            \n",
    "        #encode xclass to feature space\n",
    "        xclass = torch.Tensor(xclass)\n",
    "        xclass = xclass.to(device)\n",
    "        xclass = encoder(xclass)\n",
    "        print(xclass.shape) #torch.Size([500, 600])\n",
    "            \n",
    "        xclass = xclass.detach().cpu().numpy()\n",
    "        n = imbal[0] - imbal[i]\n",
    "        xsamp, ysamp = G_SM1(xclass,yclass,n,i)\n",
    "        print(xsamp.shape) #(4500, 600)\n",
    "        print(len(ysamp)) #4500\n",
    "        ysamp = np.array(ysamp)\n",
    "        print(ysamp.shape) #4500   \n",
    "    \n",
    "        \"\"\"to generate samples for resnet\"\"\"   \n",
    "        xsamp = torch.Tensor(xsamp)\n",
    "        xsamp = xsamp.to(device)\n",
    "        #xsamp = xsamp.view(xsamp.size()[0], xsamp.size()[1], 1, 1)\n",
    "        #print(xsamp.size()) #torch.Size([10, 600, 1, 1])\n",
    "        ximg = decoder(xsamp)\n",
    "\n",
    "        ximn = ximg.detach().cpu().numpy()\n",
    "        print(ximn.shape) #(4500, 3, 32, 32)\n",
    "        #ximn = np.expand_dims(ximn,axis=1)\n",
    "        print(ximn.shape) #(4500, 3, 32, 32)\n",
    "        resx.append(ximn)\n",
    "        resy.append(ysamp)\n",
    "        #print('resx ',resx.shape)\n",
    "        #print('resy ',resy.shape)\n",
    "        #print()\n",
    "    \n",
    "    resx1 = np.vstack(resx)\n",
    "    resy1 = np.hstack(resy)\n",
    "    #print(resx1.shape) #(34720, 3, 32, 32)\n",
    "    #resx1 = np.squeeze(resx1)\n",
    "    print(resx1.shape) #(34720, 3, 32, 32)\n",
    "    print(resy1.shape) #(34720,)\n",
    "\n",
    "    resx1 = resx1.reshape(resx1.shape[0],-1)\n",
    "    print(resx1.shape) #(34720, 3072)\n",
    "    \n",
    "    dec_x1 = dec_x.reshape(dec_x.shape[0],-1)\n",
    "    print('decx1 ',dec_x1.shape)\n",
    "    combx = np.vstack((resx1,dec_x1))\n",
    "    comby = np.hstack((resy1,dec_y))\n",
    "\n",
    "    print(combx.shape) #(45000, 3, 32, 32)\n",
    "    print(comby.shape) #(45000,)\n",
    "\n",
    "    ifile = '.../MNIST/trn_img_f/' + \\\n",
    "        str(m) + '_trn_img.txt'\n",
    "    np.savetxt(ifile, combx)\n",
    "    \n",
    "    lfile = '.../MNIST/trn_lab_f/' + \\\n",
    "        str(m) + '_trn_lab.txt'\n",
    "    np.savetxt(lfile,comby) \n",
    "    print()\n",
    "\n",
    "t1 = time.time()\n",
    "print('final time(min): {:.2f}'.format((t1 - t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
